% Copyright 2011-2015 David Hadka.  All Rights Reserved.
%
% This file is part of the MOEA Framework User Manual.
%
% Permission is granted to copy, distribute and/or modify this document under
% the terms of the GNU Free Documentation License, Version 1.3 or any later
% version published by the Free Software Foundation; with the Invariant Section
% being the section entitled "Preface", no Front-Cover Texts, and no Back-Cover
% Texts.  A copy of the license is included in the section entitled "GNU Free
% Documentation License".

\chapter{Comparative Studies}
One of the primary purposes of the MOEA Framework is facilitating large-scale comparative studies and sensitivity analysis.  Such studies demand large computational resources and generate massive amounts of data.  The \texttt{org.moeaframework.analysis.sensitivity} package contains a suite of tools for performing large-scale comparative studies.  This chapter motivates the use of comparative studies and provides the necessary details to use the tools provided by the MOEA Framework.

Academic uses of this work should cite the following paper:

\begin{quote}
Hadka, D. and Reed, P.  ``Diagnostic Assessment of Search Controls and Failure Modes in Many-Objective Evolutionary Optimization.''  Evolutionary Computation, 20(3):423-452, 2012.
\end{quote}

\section{What are Comparative Studies?}

Is algorithm A better than algorithm B?  This is the fundamental question answered by comparative studies.  Many early studies would select a problem, solve it using algorithms A and B, and compare their results to determine which produced the best result.  There are many factors at play, however, that effect the outcome of such studies.

\paragraph{Problem Domain}
The performance of an algorithm can vary widely depending on the problem being solved.  Metaheuristics are intended to be applicable over a large number of problems with varying characteristics.  Selecting a suite of test problems that capture a range of problem characteristics is important.  Several such test suites have been developed in the literature and are provided by the MOEA Framework.  These include the ZDT, DTLZ, WFG and CEC 2009 test problem suites.

\paragraph{Goals of Optimization}
In multiobjective optimization, there is no single measure of performance.  Additionally, the definition of good performance may vary from person to person.  In general, the three main goals of multiobjective optimization is proximity, diversity, and consistency.  An algorithm that produces results satisfying all three goals is ideal.  Results produced by such an algorithm will be close to the ideal result (proximity), capture the tradeoffs among competing objectives (diversity), and discover all regions of the ideal result (consistency).

To analytically capture these various goals, we use a number of performance metrics from the literature.  These include hypervolume, generational distance, inverted generational distance, spacing, and additive $\epsilon$-indicator.  See the reference text by Coello Coello et al. (2007) for details of each performance metric.

\paragraph{Parameterization}
The parameters used to configure optimization algorithms, and in particular MOEAs, can significantly impact their behavior.  Early comparative studies used the suggested parameters from the literature, but more recent theoretical and experimental results have shown that the ideal parameterization changes across problems.  To eliminate any assumptions or bias, the strategy used by the MOEA Framework is to sample across all feasible parameterizations.  Doing so allows us to 1) determine the best parameterization for each problem; 2) determine the volume of the parameterization ``sweet spot''; and 3) analyze the sensitivities of the various parameters and their impact on the overall behavior of an algorithm.

\paragraph{Algorithm Selection}
Lastly, it is important to use relevant, state-of-the-art algorithms in comparative studies.  The MOEA Framework provides both older, baseline algorithms and an assortment of modern, state-of-the-art MOEAs.  If you are proposing a new optimization algorithm, you should consider performing a large-scale comparative study against a number of state-of-the-art optimization algorithms to assess its performance relative to established algorithms.

Once the experimental design is set, you can begin running the experiments and collecting data.  The MOEA Framework provides all of the tools for these analyses.  For large-scale comparative studies, one should consider the available computing resources.  The MOEA Framework can run in nearly any computing environment, from desktop computers to massive supercomputers.  Regardless of the computing environment, the following sections walk through all the steps needed to perform a complete comparative study.

\section{Executing Commands}
In the examples below, we provide commands which are to be executed in the terminal or command prompt.  For clarity, we left out certain parts of the command and split the command across multiple lines.  The basic syntax for commands throughout this chapter is:

\begin{lstlisting}[language=Plaintext]
java CommandName
		--argument1 value1
		--argument2 value2
\end{lstlisting}

When typing these commands into the terminal or command prompt, the command should be typed on a single line.  Furthermore, Java requires two additional arguments.  First, add \texttt{-Djava.ext.dirs=lib} to specify the location of the MOEA Framework libraries.  Second, add \texttt{-Xmx512m} to specify the amount of memory allocated to Java.  In this example, 512 MBs are allocated.  512 MBs is typically sufficient, but you may decrease or increase the allocated memory as required.  The full command would be typed into the terminal or command prompt as follows:

\begin{lstlisting}[language=Plaintext]
java -Djava.ext.dirs=lib -Xmx512m CommandName
		--argument1 value1
		--argument2 value2
\end{lstlisting}

\begin{tip}
All commands provided by the MOEA Framework support the \texttt{--help} flag, which when included will print out documentation detailing the use of the command.  Use the \texttt{--help} flag to see what arguments are available and the format of the argument values, if any.  For example, type the following for any command to see its help documentation.
\end{tip}

\begin{lstlisting}[language=Plaintext]
java -Djava.ext.dirs=lib -Xmx512m CommandName --help
\end{lstlisting}

All arguments have long and short versions.  The long version is preceded by two dashes, such as \texttt{--input}.  The short version is a single dash followed by a single character, such as \texttt{-i}.   See the \texttt{--help} documentation for the long and short versions for all arguments.

The end of this chapter includes a complete Unix script for executing all commands discussed throughout this chapter.

\section{Parameter Description File}
The parameter description file describes, for each algorithm, the parameters to be sampled and their feasible ranges.  Each row in the file lists the name of the parameter, its minimum value and its maximum value.  For example, the parameter description file for NSGA-II looks like:

\begin{lstlisting}[language=Plaintext]
maxEvaluations 10000 1000000
populationSize 10 1000
sbx.rate 0.0 1.0
sbx.distributionIndex 0.0 500.0
pm.rate 0.0 1.0
pm.distributionIndex 0.0 500.0
\end{lstlisting}

The parameter names must match the parameters listed in the documentation for the algorithm.  For this example, this file is located at \file{NSGAII\_Params}.

\section{Generating Parameter Samples}
\label{sect:generateParameters}
Next, the parameter sample file must be generated.  The parameter sample file contains the actual parameterizations used when executing an algorithm.  For example, 1000 Latin hypercube samples can be generated for our NSGA-II example with the following command:

\begin{lstlisting}[language=Plaintext]
java org.moeaframework.analysis.sensitivity.SampleGenerator
		--method latin
		--numberOfSamples 1000
		--parameterFile NSGAII_Params
		--output NSGAII_Latin
\end{lstlisting}

If you are planning on performing Sobol’ global variance decomposition (discussed later), then the \texttt{saltelli} sampling method must be used.  Otherwise, \texttt{latin} is the recommended method.

\section{Evaluation}
Evaluation is the time-consuming step, as each parameter sample must be executed by the algorithm.  The evaluation process reads each line from the parameter file, configures the algorithm with those parameters, executes the algorithm and saves the results to a result file.  This result file contains the approximation sets produced by each run of the algorithm.  Entries in the result file align with the corresponding parameter sample.  For example, since we generated 1000 parameter samples in the prior step, the result file will contain 1000 approximation sets when evaluation completes.

Furthermore, to improve the statistical quality of the results, it is a common practice to repeat the evaluation of each parameter sample multiple times using different pseudo-random number generator seeds.  Stochastic search algorithms like MOEAs require randomness in several key areas, including 1) generating the initial search population; 2) selecting the parent individuals from the search population; 3) determining which decision variables to modify and the extent of the modification; and 4) determining which offspring survive to the next generation.  In some cases, these sources of randomness can significantly impact the algorithm’s behavior.  Replicating the evaluations using multiple random seeds increases the statistical robustness of the results.

The following command runs a single seed.  Note the naming convention used for the output files.  The overall filename format used in these examples for result files is \texttt{\{algorithm\}\_\{problem\}\_\{seed\}} with the \texttt{.set} extension to indicate result files containing approximation sets.  It is not necessary to follow this convention, but doing so is extremely helpful for organizing the files.

\begin{lstlisting}[language=Plaintext]
java org.moeaframework.analysis.sensitivity.Evaluator
		--parameterFile NSGAII_Params
		--input NSGAII_Latin
		--seed 15
		--problem DTLZ2_2
		--algorithm NSGAII
		--output NSGAII_DTLZ2_2_15.set
\end{lstlisting}

This command will be invoked once for each seed, changing the \texttt{--seed} value and the \texttt{--output} filename each time.  Using at least 50 seeds is recommended unless the computational demands are prohibitive.

\section{Check Completion}
Double-checking that all result files are complete (all parameter samples have been successfully evaluated) is an important step to prevent analyzing incomplete data sets.  The following command prints out the number of approximation sets contained in each result file.

\begin{lstlisting}[language=Plaintext]
java org.moeaframework.analysis.sensitivity.ResultFileInfo
		--problem DTLZ2_2
		NSGAII_DTLZ2_2_*.set
\end{lstlisting}

Since our example used 1000 parameter samples, each result file should contain 1000 approximation sets.

\begin{lstlisting}[language=Plaintext]
NSGAII_DTLZ2_2_0.set 1000
NSGAII_DTLZ2_2_1.set 1000
NSGAII_DTLZ2_2_2.set 952
NSGAII_DTLZ2_2_3.set 1000
...
\end{lstlisting}

In this example, we see that \file{NSGAII\_DTLZ2\_2\_2.set} is incomplete since the result file only contains 952 entries.  Incomplete files occur when the evaluation step is interrupted, such as when the evaluation process is terminated when it exceeds its allocated wall-clock time.  The evaluation process supports resuming execution for this very reason.  Simply call the Evaluator command again on the incomplete seed, and it will automatically resume evaluation where it left off.

\section{Reference Set Generation}
\label{sect:referenceSetGeneration}
Many performance metrics require the Pareto optimal set.  For example, a metric may measure the distance of the approximation set produced by an algorithm to the Pareto optimal set.  A smaller distance indicates the algorithm finds closer approximations of the Pareto optimal set.

If the true Pareto optimal set for a problem is known a priori, then this step may be skipped.  Many real-world problems, however, do not have a defined true Pareto optimal set.  For such cases, it is a common practice to use the best known approximation of the Pareto optimal set as the reference set.  This best known approximation consists of all Pareto optimal solutions produced by the optimization algorithms.

Continuing with our example, the best known approximation for the reference set can be produced by combining the approximation sets produced by all algorithms, NSGA-II and GDE3 in this example, across all seeds.

\begin{lstlisting}[language=Plaintext]
java org.moeaframework.analysis.sensitivity.ResultFileMerger
		--problem DTLZ2_2
		--output DTLZ2_2.reference
		NSGAII_DTLZ2_2_*.set GDE3_DTLZ2_2_*.set
\end{lstlisting}

When using the true Pareto optimal set when calculating performance metrics (discussed in the following section), the results are said to be \emph{absolute}.  Using the best known approximation produces \emph{relative} performance metrics.

\section{Metric Calculation}
Given the reference set for the problem, we can now calculate the performance metrics.  Recall that the result file contains an approximation set for each parameter sample.  By calculating the performance metrics for each approximation set, we deduce the absolute or relative performance  for each parameter sample.  The following command would be invoked for each seed by varying the input and output filenames appropriately.

\begin{lstlisting}[language=Plaintext]
java org.moeaframework.analysis.sensitivity.ResultFileEvaluator
		--reference DTLZ2_2.reference
		--input NSGAII_DTLZ2_2_15.set
		--problem DTLZ2_2
		--output NSGAII_DTLZ2_2_15.metrics
\end{lstlisting}

Note the use of our filename convention with the \texttt{.metrics} extension to indicate a file containing performance metric results.  Each row in this file contains the performance metrics for a single approximation set.  The performance metrics on each line appear in the order: 

\vspace{10pt}
\begin{center}
\begin{tabular}{cl}
  Column & Performance Metric\\
  \hline
  0 & Hypervolume\\
  1 & Generational Distance\\
  2 & Inverted Generational Distance\\
  3 & Spacing\\
  4 & Additive $\epsilon$-Indicator\\
  5 & Maximum Pareto Front Error\\
\end{tabular}
\end{center}

\section{Averaging Metrics}
When multiple seeds are used, it is useful to aggregate the performance metrics across all seeds.  For this example, we compute the average of performance metrics with the following command.

\begin{lstlisting}[language=Plaintext]
java org.moeaframework.analysis.sensitivity.SimpleStatistics
		--mode average
		--output NSGAII_DTLZ2_2.average
		NSGAII_DTLZ2_2_*.metrics
\end{lstlisting}

\section{Analysis}
Finally, we can begin analyzing the data.  The first analytical step is to generate descriptive statistics for the data.  Three common statistics are the best achieved result, probability of attainment, and efficiency.  The following command is used to calculate these statistics.

\begin{lstlisting}[language=Plaintext]
java org.moeaframework.analysis.sensitivity.Analysis
		--parameterFile NSGAII_Params
		--parameters NSGAII_Latin
		--metric 1
		--threshold 0.75
		--efficiency
		NSGAII_DTLZ2_2.average
\end{lstlisting}

Note that the \texttt{--metric} argument specifies which of the six performance metrics are used when calculating the results.  In this example, \texttt{--metric 1} selects the generational distance metric.  The output of this command will appear similar to:

\begin{lstlisting}[language=Plaintext]
NSGAII_DTLZ2_2.average:
		Best: 0.98
		Attainment: 0.53
		Efficiency: 0.38
\end{lstlisting}

The interpretation of each statistic is explained in detail below.

\subsection{Best}
The best attained value measures the absolute best performance observed across all parameters.  The value is normalized so that 1.0 indicates the best possible result and 0.0 indicates the worst possible result.  In the example output, a best achieved value of 0.98 indicates at least one parameter setting produced a near-optimal generational distance (within 2\% of the optimum).

\subsection{Attainment}
While an optimization algorithm may produce near-optimal results, it will be useless unless it can consistently produce good results.  The probability of attainment measures the reliability of an algorithm.  Recall that we used the \texttt{--threshold 0.75} argument when invoking the command above.  The probability of attainment measures the percentage of parameter samples which meet or exceed this threshold.  The threshold can be varied from 0.0 to 1.0.  In the example output, a probability of attainment of 0.53 indicates only half of the parameter samples reached or exceeded the 75\% threshold.

\subsection{Efficiency}
Another important consideration is the computational resources required by an optimization algorithm.  An algorithm which can quickly produce near-optimal results is preferred over an algorithm that runs for an eternity.  Efficiency is a measure of the number of function evaluations (NFE) required to reach the threshold with high likelihood.  Efficiency values are normalized so that 1.0 represents the most efficient algorithm and 0.0 indicates the least efficient.

In this example, NSGA-II reports an efficiency of 0.38.  Recall that we set the upper bound for \texttt{maxEvaluations} to 1000000 in the parameter description file for NSGA-II.  This implies that it requires at least $(1.0 - 0.38) * 1000000 = 620000$ NFE to reach the 75\% threshold with high likelihood.  

Since the efficiency calculation may be time consuming, you must specify the \texttt{--efficiency} flag in order to calculate efficiency.  There is a fourth statistic called controllability, which is enabled by the \texttt{--controllability} flag, but its use is outside the scope of this manual.

\section{Set Contribution}
If multiple algorithms were executed, it is possible to calculate what percentage of the reference set was contributed by each algorithm.  Optimization algorithms that contribute more to the reference set are considered better, as such algorithms are producing the best known solutions to the problem.

First, we generate the combined approximation set for each algorithm.  This combined approximation set is similar to the reference set, but is generated for a single algorithm.  It represents the best known approximation set that each algorithm is capable of producing.  For our example, the following two commands produce the combined approximation sets for NSGA-II and GDE3, respectively.

\begin{lstlisting}[language=Plaintext]
java org.moeaframework.analysis.sensitivity.ResultFileMerger
		--problem DTLZ2_2
		--output NSGAII_DTLZ2_2.set
		NSGAII_DTLZ2_2_*.combined

java org.moeaframework.analysis.sensitivity.ResultFileMerger
		--problem DTLZ2_2
		--output GDE3_DTLZ2_2.set
		GDE3_DTLZ2_2_*.combined
\end{lstlisting}

Next, invoke the following command to determine the percentage of the reference set each algorithm contributed.

\begin{lstlisting}[language=Plaintext]
java org.moeaframework.analysis.sensitivity.SetContribution
		--reference DTLZ2_2.reference
		NSGAII_DTLZ2_2.combined GDE3_DTLZ2_2.combined
\end{lstlisting}

For example, the following output indicates NSGA-II contributed 71\% of the approximation set and GDE3 contributed 29\%.

\begin{lstlisting}[language=Plaintext]
NSGAII_DTLZ2_2.combined 0.71
GDE3_DTLZ2_2.combined 0.29
\end{lstlisting}

Note that it is possible for the percentages to sum to more than 1 if the contributions of each optimization algorithm overlap.

\section{Sobol Analysis}
The last type of analysis provided by the MOEA Framework is Sobol’ global variance decomposition.  Sobol’ global variance decomposition is motivated by the need to understand the factors which control the behavior of optimization algorithms.  The outcome of optimization is ultimately controlled by four factors:

\begin{enumerate}
\item the problem being solved;
\item the fundamental characteristics of the optimization algorithm;
\item the parameters used to configure the optimization algorithm; and
\item random seed effects.
\end{enumerate}

The impact of the problem is largely outside our control.  Harder problems are simply harder to solve.  But, its impact can be mitigated by selecting an appropriate optimization algorithm and carefully configuring its parameters.

How an optimization algorithm works, the way it stochastically samples the search space, how its decision variables are encoded, and its ability to cope with different search landscapes can greatly impact the outcome of optimization.  Selecting an optimization algorithm appropriate for the problem domain is important.  This selection is generally left to the end user, but should be influenced by results from comparative studies.

Once an appropriate optimization algorithm is selected, it can be fine-tuned by controlling its various parameters.  Understanding how parameters impact an algorithm is important.  Poor parameterization will lead to poor performance.  For example, too small a population size can lead to preconvergence and loss of diversity, whereas too large of a population size may unnecessarily slow search by wasting resources.

The last factor to impact optimization algorithms are random seed effects.  An algorithm whose performance varies widely across different random seeds is unreliable, and will require many replications in order to guarantee high-quality results.  This is why we recommend using at least 50 seeds when performing any type of comparative study, as averaging across many seeds mitigates the impact of random seed effects.

Sobol’ global variance decomposition helps us understand the impact of parameterization.  It identifies which parameters are important (i.e., which parameters cause the largest variation in performance).  Additionally, it identifies interactions between parameters.  For example, using a larger population size may increase the NFE needed to achieve high-quality results.  This is a second-order interaction (involving two parameters) that can be identified using Sobol’ global variance decomposition.

In order to perform Sobol’ global variance decomposition, you must use the \texttt{saltelli} sampling method when generating the parameter samples (see \sectref{sect:generateParameters} for details).

Similar to the earlier analysis method, the performance metric must be specified with the \texttt{--metric} argument.  In this example, we use \texttt{--metric 0} to select the hypervolume metric.  The following command calculates the parameter sensitivities for NSGA-II.

\begin{lstlisting}[language=Plaintext]
java org.moeaframework.analysis.sensitivity.SobolAnalysis
		--parameterFile NSGAII_Params
		--input NSGAII_DTLZ2_2.average
		--metric 0
\end{lstlisting}

The output from this command will appear similar to the following.

\begin{lstlisting}[language=Plaintext]
First-Order Effects
  populationSize 0.15 [0.04]
  maxEvaluations 0.45 [0.03]
  ...
Total-Order Effects
  populationSize 0.49 [0.06]
  maxEvaluations 0.83 [0.05]
  ...
Second-Order Effects
  populationSize * maxEvaluations 0.21 [0.04]
  ...
\end{lstlisting}

Three groupings are reported: first-order effects, second-order effects, and total-order effects.  First-order effects describe the sensitivities of each parameter in isolation.  Second-order effects describe the pairwise interaction between parameters.  Total-order effects describe the sum of all sensitivities attributed to each parameter.  Each row lists the parameter(s), its sensitivity as a percentage, and the bootstrap confidence interval for the sensitivities in brackets.

In this example, we see \texttt{maxEvaluations} has the largest impact, accounting for nearly half (45\%) of the first-order sensitivities.  The large total-order effects (83\%) indicate \texttt{maxEvaluations} interacts strongly with other parameters.  There is a moderate level of interaction between \texttt{populationSize} and \texttt{maxEvaluations} (21\%).  Note that this analysis does not tell us how the parameters interact, it simply indicates the existence of interaction.  To see the interactions in more detail, it is often helpful to generate a contour plot with the x and y-axes representing two parameters and the height/color representing performance.

\section{Example Script File (Unix/Linux)}
The following script demonstrates how the commands introduced throughout this chapter work together.  All that is needed to start using this script is creating the \file{NSGAII\_Params} and \file{GDE3\_Params} parameter description files.  Note, however, that since the number of samples (\texttt{NSAMPLES}) and number of replications (\texttt{NSEEDS}) are large, this script will take a while to run.  You may also modify the parameter sampling method (\texttt{METHOD}), the problem being tested (\texttt{PROBLEM}), and the list of algorithms being compared (\texttt{ALGORITHMS}).

\begin{lstlisting}[language=bash,breakatwhitespace=true]
NSAMPLES=1000
NSEEDS=50
METHOD=Saltelli
PROBLEM=UF1
ALGORITHMS=( NSGAII GDE3 )

SEEDS=$(seq 1 ${NSEEDS})
JAVA_ARGS="-Djava.ext.dirs=lib -Xmx512m"
set -e

# Clear old data
echo -n "Clearing old data (if any)..."
rm *_${PROBLEM}_*.set
rm *_${PROBLEM}_*.metrics
echo "done."

# Generate the parameter samples
echo -n "Generating parameter samples..."
for ALGORITHM in ${ALGORITHMS[@]}
do
  java ${JAVA_ARGS} org.moeaframework.analysis.sensitivity.SampleGenerator -m ${METHOD} -n ${NSAMPLES} -p ${ALGORITHM}_Params -o ${ALGORITHM}_${METHOD}
done
echo "done."

# Evaluate all algorithms for all seeds
for ALGORITHM in ${ALGORITHMS[@]}
do
  echo "Evaluating ${ALGORITHM}:"
  for SEED in ${SEEDS}
  do
    echo -n "  Processing seed ${SEED}..."
    java ${JAVA_ARGS} org.moeaframework.analysis.sensitivity.Evaluator -p ${ALGORITHM}_Params -i ${ALGORITHM}_${METHOD} -b ${PROBLEM} -a ${ALGORITHM} -s ${SEED} -o ${ALGORITHM}_${PROBLEM}_${SEED}.set
    echo "done."
  done
done

# Generate the combined approximation sets for each algorithm
for ALGORITHM in ${ALGORITHMS[@]}
do
  echo -n "Generating combined approximation set for ${ALGORITHM}..."
  java ${JAVA_ARGS} org.moeaframework.analysis.sensitivity.ResultFileMerger -b ${PROBLEM} -o ${ALGORITHM}_${PROBLEM}.combined ${ALGORITHM}_${PROBLEM}_*.set
  echo "done."
done

# Generate the reference set from all combined approximation sets
echo -n "Generating reference set..."
java ${JAVA_ARGS} org.moeaframework.util.ReferenceSetMerger -o ${PROBLEM}.reference *_${PROBLEM}.combined > /dev/null
echo "done."

# Evaluate the performance metrics
for ALGORITHM in ${ALGORITHMS[@]}
do
  echo "Calculating performance metrics for ${ALGORITHM}:"
  for SEED in ${SEEDS}
  do
    echo -n "  Processing seed ${SEED}..."
    java ${JAVA_ARGS} org.moeaframework.analysis.sensitivity.ResultFileEvaluator -b ${PROBLEM} -i ${ALGORITHM}_${PROBLEM}_${SEED}.set -r ${PROBLEM}.reference -o ${ALGORITHM}_${PROBLEM}_${SEED}.metrics
    echo "done."
  done
done

# Average the performance metrics across all seeds
for ALGORITHM in ${ALGORITHMS[@]}
do
  echo -n "Averaging performance metrics for ${ALGORITHM}..."
  java ${JAVA_ARGS} org.moeaframework.analysis.sensitivity.SimpleStatistics -m average -o ${ALGORITHM}_${PROBLEM}.average ${ALGORITHM}_${PROBLEM}_*.metrics
  echo "done."
done

# Perform the analysis
echo ""
echo "Analysis:"
for ALGORITHM in ${ALGORITHMS[@]}
do
  java ${JAVA_ARGS} org.moeaframework.analysis.sensitivity.Analysis -p ${ALGORITHM}_Params -i ${ALGORITHM}_${METHOD} -m 1 ${ALGORITHM}_${PROBLEM}.average
done

# Calculate set contribution
echo ""
echo "Set contribution:"
java ${JAVA_ARGS} org.moeaframework.analysis.sensitivity.SetContribution -r ${PROBLEM}.reference *_${PROBLEM}.combined

# Calculate Sobol sensitivities
if [ ${METHOD} == "Saltelli" ]
then
  for ALGORITHM in ${ALGORITHMS[@]}
  do
    echo ""
    echo "Sobol sensitivities for ${ALGORITHM}"
    java ${JAVA_ARGS} org.moeaframework.analysis.sensitivity.SobolAnalysis -p ${ALGORITHM}_Params -i ${ALGORITHM}_${PROBLEM}.average -m 1
  done
fi
\end{lstlisting}

\section{PBS Job Scripting (Unix)}
It is possible to speed up the execution of a comparative study if you have access to a cluster or supercomputer.  The following script demonstrates automatically submitting jobs to a Portable Batch System (PBS).  PBS is a program which manages job execution on some clusters and supercomputers and allows us to distribute the workload across multiple processors.

\begin{lstlisting}[language=bash,breakatwhitespace=true]
for ALGORITHM in ${ALGORITHMS[@]}
do
  for SEED in ${SEEDS}
  do
    NAME=${ALGORITHM}_${PROBLEM}_${SEED}
    PBS="\
#PBS -N ${NAME}\n\
#PBS -l nodes=1\n\
#PBS -l walltime=96:00:00\n\
#PBS -o output/${NAME}\n\
#PBS -e error/${NAME}\n\
cd \$PBS_O_WORKDIR\n\
java ${JAVA_ARGS} org.moeaframework.analysis.sensitivity.Evaluator -p ${ALGORITHM}_Params -i ${ALGORITHM}_${METHOD} -b ${PROBLEM} -a ${ALGORITHM} -s ${SEED} -o ${NAME}.set"
    echo -e $PBS | qsub
  done
done
\end{lstlisting}

Note that the above script sets a walltime of 96 hours.  You should adjust this value according to the walltime limit on your particular PBS queue.  Jobs will be terminated by the PBS system if their wall-clock time exceeds this time limit.  After all jobs terminate, use \texttt{ResultFileInfo} to check if the results are complete.  If any results are incomplete, simply rerun the script above.  The Evaluator automatically resumes processing where it left off.

In summary, the execution of a comparative study using PBS will generally follow these steps:

\begin{enumerate}
\item Create the parameter description files
\item Generate the parameter samples
\item Submit the evaluation jobs to PBS and wait for them to finish
\item Check if the results are complete
\begin{enumerate}
\item If complete, continue to step 5
\item If incomplete, repeat step 3
\end{enumerate}
\item Generate approximation sets
\item Generate reference set (if one is not available)
\item Calculate the performance metrics
\item Analyze the results
\end{enumerate}

\section{Conclusion}
This chapter introduced techniques for performing comparative studies between two or more optimization algorithms.  Using these techniques is strongly encouraged since they ensure results are rigorous and statistically sound.  The final section in this chapter, \sectref{sect:troubleshooting}, includes troubleshooting steps if you encounter issues while using any of the tools discussed in this chapter.

\section{Troubleshooting}
\label{sect:troubleshooting}
\noindent
\textit{The Evaluator or Analysis command throws an error saying ``maxEvaluations not defined.''}

\begin{quote}
The Evaluator requires the \texttt{maxEvaluations} parameter to be defined.  \texttt{maxEvaluation}s can either be included in the parameter sampling by including an entry in the parameter description file, or by setting \texttt{maxEvaluations} to a fixed value for all samples using the \texttt{-x maxEvaluations=\{value\}} argument.
\end{quote}

\noindent
\textit{The Analysis command throws an error saying ``requires hypervolume option.''}

\begin{quote}
When analyzing results using the hypervolume metric (\texttt{--metric 0}), it is necessary to also include the \texttt{--hypervolume \{value\}} argument to set the maximum hypervolume for the problem.
\end{quote}

\noindent
\textit{The Evaluator or ResultFileEvaluator command throws an error saying ``input appears to be newer than output.''}

\begin{quote}
The Evaluator and ResultFileEvaluator read entries in an input file and write the corresponding outputs to a separate output file.  If the last modified date on the input file is newer than the date on the output file, this error is thrown.  This error suggests that the input file has been modified unexpectedly, and attempting to resume with a partially evaluated output file may result in inconsistent results.

If you can confirm that the input file has not been changed, then add the \texttt{--force} flag to the command to override this check.  

However, if the input file has been modified, then you must delete the output file and restart evaluation from the beginning.  Do not attempt to resume evaluation if the input file has changed.
\end{quote}

\noindent
\textit{The Evaluator or ResultFileEvaluator command throws an error saying ``output has more entries than input.''}

\begin{quote}
This error occurs when the output file contains more entries than the input file, which indicates an inconsistency between the two files.  The output file should never have more entries than the input file.  You must delete the output file and restart evaluation from the beginning.
\end{quote}

\noindent
\textit{I get an error saying ``no reference set available.''}

\begin{quote}
Several of the commands described in this section require a reference set.  Some problems provide a default reference set.  If a reference set is required and the problem does not provide a default reference set, then you will see this error.  You must manually provide a reference set using the \texttt{--reference} argument.  See \sectref{sect:referenceSetGeneration} for details.
\end{quote}

\noindent
\textit{I get an error saying ``unable to load reference set.''}

\begin{quote}
This error occurs when the reference set file is missing, could not be accessed, or is corrupted.  The error message should include additional details describing the cause of the error.  Typically, you will need to change the \texttt{--reference} argument to point to a valid reference set file.
\end{quote}

\noindent
\textit{Sobol’ global variance decomposition is reporting large bootstrap confidence intervals.}

\begin{quote}
Small bootstrap confidence intervals (5\% or less) are desired.  A large bootstrap confidence interval often indicates that an insufficient number of samples were used.  Increasing the number of parameter samples will likely shrink the confidence intervals and improve the reliability of the results.

Large bootstrap confidence intervals may also arise under certain conditions which cause numerical instability, such as division by values near zero.  Addressing this source of error is outside the scope of this manual.
\end{quote}

\noindent
\textit{I received one of the following errors: ``insufficient number of entries in row,'' ``invalid entry in row,'' or ``parameter out of bounds.''}

\begin{quote}
These errors indicate issues with the parameter samples.  If any of these errors occurs, it likely indicates that the parameter description file has been modified and is no longer consistent with the parameter samples.  ``Insufficient number of entries in row'' occurs when the number of parameters in the parameter description file and the parameter samples file do not match (e.g., there are missing parameters).  ``Invalid entry in row'' indicates one of the parameter samples could not be parsed and is likely corrupted.  ``Parameter out of bounds'' indicates one of the parameter samples contained a value that exceeded the bounds defined in the parameter description file.

If you intentionally modified the parameter description file, then you will need to delete the old parameter samples (and any old result files) and restart from the beginning.

If you did not recently modify the parameter description file, then the data is likely corrupted.  Revert to a backup if possible; otherwise, you will need to delete the old parameter samples (and any old result files) and restart from the beginning.
\end{quote}

\noindent
\textit{I get an error saying ``expected only three items per line.''}

\begin{quote}
The parameter description file is improperly formatted.  Each row in the file should contain exactly three items separated by whitespace.  The items in order are the parameter name, its minimum bound and its maximum bound.  The parameter name must be a single word (no whitespace).
\end{quote}

\noindent
\textit{The SimpleStatistics command throws one of the following errors: ``requires at least one file,'' ``empty file,'' ``unbalanced rows,'' or ``unbalanced columns.''}

\begin{quote}
SimpleStatistics aggregates results across multiple files.  In order to correctly aggregate the results, a number of conditions must be met.  First, there must be at least one data file to aggregate, otherwise the ``requires at least one file'' error occurs.  Second, each file must contain an equal number of rows and columns.  If any file does not satisfy this condition, one of the ``empty file,'' ``unbalanced rows,'' or ``unbalanced columns'' errors will occur.  The error message identifies the responsible file.

The occurrence of any of these errors indicates that one of the evaluation steps was either skipped or did not complete fully.  Generally, you can correct this error by resuming the evaluation of any incomplete files.
\end{quote}
